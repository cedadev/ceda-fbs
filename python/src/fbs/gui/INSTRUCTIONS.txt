# Instructions on setting up and running an archive scan

export BASEDIR=/group_workspaces/jasmin/cedaproc/$USER/git/ceda-fbs
mkdir -p $BASEDIR
cd $BASEDIR/

wget https://raw.githubusercontent.com/cedadev/ceda-fbs/master/install-ceda-fbs.sh
chmod 750 install-ceda-fbs.sh
./install-ceda-fbs.sh
mkdir datasets
mkdir logs

cat << EOF >> setup_env.sh
#!/bin/bash

export BASEDIR=/group_workspaces/jasmin/cedaproc/$USER/git/ceda-fbs
cd $BASEDIR
. venv-ceda-fbs/bin/activate
cd ceda-fbs/python/src/fbs/gui
export PYTHONPATH=$BASEDIR/ceda-fbs/python/src/fbs:$BASEDIR/ceda-fbs/python:$PYTHONPATH
EOF

chmod 750 setup_env.sh
. setup_env.sh

# EDIT THE RELEVANT INI FILE
vi ../../../config/ceda_fbs.ini

./create_datasets_ini_file.sh

# Make file lists for all datasets:
#  - this will write every file path in the archive to big test files
#    (one per dataset)
# It runs on LOTUS and will take about 1 day
python make_file_lists.py -f ceda_all_datasets.ini -m $BASEDIR/datasets --host lotus -p 256

# Now create scripts to scan the archive. Each job will scan 10,000 files so we create
# a long list of commands to submit jobs to LOTUS
python scan_archive.py --filename $BASEDIR/datasets --num-files 10000 --level 2 --host lotus

# which writes: lotus_commands.txt

# Now we run the long-running job that will keep submitting jobs to LOTUS until they have
# all run. It limits the number of jobs to 128 at any one time.
# This will take > 4 weeks to run!
nohup python run_commands_in_lotus.py -f lotus_commands.txt -p 128 > /dev/null 2>&1 &

# Wait to check there are some results in the Index (once it has been created).
# Check this returns a count of more than one

curl -XGET jasmin-es1.ceda.ac.uk:9200/ceda-archive-level-2/_count

# Then remove replicate indexes so that only one is created (to save memory)
curl -XPUT jasmin-es1.ceda.ac.uk:9200/ceda-archive-level-2/_settings -d '{  "number_of_replicas": 0 }'
